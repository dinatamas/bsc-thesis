from copy import copy

from grammar.grammar import *

def generate_parser(grammar, outbuffer):
    regexs = '\n'.join(8 * ' ' + f"re.compile(r'{pattern}'),"
                       for pattern in collect_patterns(grammar))
    outbuffer.write(boilerplate.format(
        verbatim = grammar.verbatim,
        regexs = regexs))
    parser_generator = ParserGenerator()
    parser_generator.visit(grammar, outbuffer)

# ============================================================================
# Boilerplate.
# ============================================================================

boilerplate = '''# Generated by SASN's parser_generator.
from collections import namedtuple
import re

from grammar.grammar import *
from grammar.syntax_tree import TokenNode, RuleNode
from grammar.token import Token

def parse_syntax_tree(snippet):
    tokens = Lexer.lex(snippet)
    syntax_tree = Parser.parse(tokens)
    return syntax_tree

# -----------
# Exceptions.
# -----------

class LanguageError(Exception):
    def __init__(self, msg, pos):
        super().__init__()
        self.msg = msg
        self.pos = pos

    def __str__(self):
        return f'Language error at {{self.pos}}: {{self.msg}}'

class RhsActionNoMatch(Exception):  # Won't be raised.
    def __init__(self, msg):
        super().__init__()
        self.msg = msg

# ============================================================================
# Verbatim.
# ============================================================================

{verbatim}

# ============================================================================
# Lexical analysis.
# ============================================================================

MultiToken = namedtuple('MultiToken', ('kinds', 'string', 'pos'))

# ------------------------------------

class LexerState:
    def __init__(self, description):
        self.buffer = description
        self.pos = (0, 0)

    def advance(self, count):
        for char in self.buffer[:count]:
            if char == '\\n':
                self.pos = (self.pos[0] + 1, 0)
            else:
                self.pos = (self.pos[0], self.pos[1] + 1)
        self.buffer = self.buffer[count:]

    def error(self, msg):
        raise LanguageError(msg, (self.pos[0] + 1, self.pos[1] + 1))

# ------------------------------------

class Lexer:
    regexs = (
{regexs}
        re.compile(r'.')
    )

    @staticmethod
    def lex(description):
        state = LexerState(description)
        while state.buffer:
            # Find longest match.
            kinds = []
            string = ''
            match_len = 0
            for pattern in Lexer.regexs:
                match = re.match(pattern, state.buffer)
                if match:
                    if match.groups():
                        curr_string = match.groups()[0]
                    else:
                        curr_string = match.group()
                    if len(match.group()) > match_len:
                        kinds = [match.re.pattern]
                        string = curr_string
                        match_len = len(match.group())
                    elif len(match.group()) == match_len:
                        kinds.append(match.re.pattern)

            # Handle matched regex.
            if r'.' in kinds:
                if len(kinds) == r'.':
                    state.error(f'Invalid character: {{repr(self.buffer[0])}}')
                kinds.remove(r'.')
            yield MultiToken(kinds, string, state.pos)
            state.advance(match_len)
        yield MultiToken([r'ENDMARKER'], '', state.pos)

# ============================================================================
# Syntactic analysis.
# ============================================================================

class ParserState:
    def __init__(self, multitokens):
        self.multitokens = iter(multitokens)
        self.curr = None
        self.finished = False
        self.pos = (0, 0)
        self.advance()
        self.action_buffer_name = None
        self.action_buffer_value = None

    def advance(self):
        try:
            self.curr = next(self.multitokens)
            self.pos = self.curr.pos
        except StopIteration:
            self.finished = True
            self.curr = None
        self.clear_action_buffer()

    def clear_action_buffer(self):
        self.action_buffer_name = None
        self.action_buffer_value = None

    def error(self, msg):
        raise LanguageError(msg, (self.pos[0] + 1, self.pos[1] + 1))

    # -------------------
    # Recognize patterns.
    # -------------------

    def peek(self, *kinds):  # Peek but do not consume.
        # Previous cached action match.
        if self.action_buffer_name:
            for kind in kinds:
                if isinstance(kind, RhsActionMark):
                    if self.action_buffer_name == kind.name:
                        return True
            return False
        for kind in kinds:
            # Peek an action.
            if isinstance(kind, RhsActionMark):
                res = globals()['parse_' + kind.name](self)
                if not isinstance(res, RhsActionNoMatch):
                    self.action_buffer_name = kind.name
                    self.action_buffer_value = res
                    return True
            if self.finished:
                break
            # Peek a pattern.
            if kind in self.curr.kinds:
                return True
        return False

    def expect(self, *kinds):  # Consume or fail.
        # Previous cached action match.
        if self.action_buffer_name:
            for kind in kinds:
                if isinstance(kind, RhsActionMark):
                    if self.action_buffer_name == kind.name:
                        self.clear_action_buffer()
                        return self.action_buffer_value
            self.error(f"Invalid token: {{repr(self.curr)}}, "
                       f"expected action '{{self.action_buffer_name}}'")
        for kind in kinds:
            # Consume an action.
            if isinstance(kind, RhsActionMark):
                res = globals()['parse_' + kind.name](self)
                if isinstance(res, RhsActionNoMatch):
                    self.error(res.msg)
                self.clear_action_buffer()
                return res
            if self.finished:
                break
            # Consume a pattern.
            elif kind in self.curr.kinds:
                token = Token(kind, self.curr.string, self.pos)
                self.advance()
                return token
        if self.finished:
            self.error(f'Unexpected EOF: '
                       f'expected one of: {{[k for k in kinds]}}')
        self.error(f'Invalid token: {{repr(self.curr)}}, '
                   f'expected one of: {{[k for k in kinds]}}')

# ------------------------------------

'''

# ============================================================================
# Lexer generation.
# ============================================================================

def collect_patterns(grammar):
    rhs_cache = set()
    pattern_cache = set()
    for rule in grammar.rules:
        collect_patterns_helper(rule.rhs, rhs_cache, pattern_cache)
    return pattern_cache

def collect_patterns_helper(rhs, rhs_cache, pattern_cache):
    if rhs not in rhs_cache:
        rhs_cache.add(rhs)
        if isinstance(rhs, RhsRegex):
            pattern_cache.add(rhs.pattern)
        elif isinstance(rhs, RhsRule):
            collect_patterns_helper(rhs.rule.rhs, rhs_cache, pattern_cache)
        elif isinstance(rhs, RhsAction):
            pass
        elif isinstance(rhs, (RhsSequence, RhsChoice)):
            for child in rhs.children:
                collect_patterns_helper(child, rhs_cache, pattern_cache)
        elif isinstance(rhs, (RhsZeroOrOne, RhsZeroOrMany, RhsOneOrMany)):
            collect_patterns_helper(rhs.child, rhs_cache, pattern_cache)
        elif isinstance(rhs, RhsSkip):
            collect_patterns_helper(rhs.child, rhs_cache, pattern_cache)

# ============================================================================
# Parser generation.
# ============================================================================

class ParserGeneratorState:
    def __init__(self, outbuffer):
        self.outbuffer = outbuffer
        self.indent_level = 0
        self.cv_cache = dict()
        self.fs_cache = dict()
        self.formatted_fs_cache = dict()
        self.wv_cache = dict()
        self.skip = False

    def emit(self, msg=''):
        self.outbuffer.write(self.indent_level * 4 * ' ' + msg + '\n')

    # ----------------------
    # Quick access wrappers.
    # ----------------------

    def first_set(self, rhs):
        if rhs not in self.formatted_fs_cache:
            # The first_set returns from the fs_cache, leave that intact.
            fs = copy(first_set(rhs, self.fs_cache, self.cv_cache))
            for i in range(len(fs)):
                if isinstance(fs[i], RhsActionMark):
                    fs[i] = f"RhsActionMark('{fs[i].name}')"
                elif isinstance(fs[i], str):
                    fs[i] = "r'" + fs[i] + "'"
            self.formatted_fs_cache[rhs] = ", ".join(fs)
        return self.formatted_fs_cache[rhs]

    def will_vanish(self, rhs):
        return will_vanish(rhs, self.wv_cache)

# ------------------------------------

class ParserGenerator(GrammarVisitorBase):
    def visitGrammar(self, grammar, outbuffer):
        state = ParserGeneratorState(outbuffer)
        state.emit('class Parser:')
        state.indent_level += 1
        state.emit('@staticmethod')
        state.emit('def parse(tokens):')
        state.indent_level += 1
        state.emit('state = ParserState(tokens)')
        state.emit(f'syntax_tree = Parser.parse_{grammar.rules[0].name}(state)')
        state.emit('return syntax_tree')
        state.emit()
        state.indent_level -= 1
        for rule in grammar.rules:
            self.visit(rule, state)
            state.emit()
        state.indent_level -= 1

    def visitRule(self, rule, state):
        state.emit('@staticmethod')
        state.emit(f'def parse_{rule.name}(state):')
        state.indent_level += 1
        state.emit('children = list()')
        self.visit(rule.rhs, state)
        state.emit(f"return RuleNode('{rule.name}', children)")
        state.indent_level -= 1

    # ------------------------------------
    # Generate parsers for Rhs variations.
    # ------------------------------------

    def visitRhsRegex(self, rhs, state):
        state.emit(f"token = state.expect(r'{rhs.pattern}')")
        if not state.skip:
            state.emit(f'children.append(TokenNode(token))')

    def visitRhsRule(self, rhs, state):
        state.emit(f'child = Parser.parse_{rhs.rule.name}(state)')
        if not state.skip and not state.will_vanish(rhs):
            state.emit(f'children.append(child)')

    def visitRhsAction(self, rhs, state):
        state.emit(f"child = state.expect(RhsActionMark('{rhs.name}'))")
        if not state.skip:
            state.emit('if child:')
            state.indent_level += 1
            state.emit(f'children.append(child)')
            state.indent_level -= 1

    def visitRhsSequence(self, rhs, state):
        for child in rhs.children:
            self.visit(child, state)

    def visitRhsChoice(self, rhs, state):
        state.emit('if False: pass')
        for child in rhs.children:
            state.emit(f'elif state.peek({state.first_set(child)}):')
            state.indent_level += 1
            self.visit(child, state)
            state.indent_level -= 1
        state.emit('else:')
        state.indent_level += 1
        state.emit((f'state.error(f"Token not in choice: {{state.curr}}, '
                    f'expected one of: " + "{state.first_set(child)}")'))
        state.indent_level -= 1

    def visitRhsZeroOrOne(self, rhs, state):
        state.emit(f'if state.peek({state.first_set(rhs.child)}):')
        state.indent_level += 1
        self.visit(rhs.child, state)
        state.indent_level -= 1

    def visitRhsZeroOrMany(self, rhs, state):
        state.emit(f'while state.peek({state.first_set(rhs.child)}):')
        state.indent_level += 1
        self.visit(rhs.child, state)
        state.indent_level -= 1

    def visitRhsOneOrMany(self, rhs, state):
        self.visit(rhs.child, state)
        state.emit(f'while state.peek({state.first_set(rhs.child)}):')
        state.indent_level += 1
        self.visit(rhs.child, state)
        state.indent_level -= 1

    def visitRhsSkip(self, rhs, state):
        previous_skip = state.skip
        state.skip = True
        self.visit(rhs.child, state)
        state.skip = previous_skip

